[
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html",
    "title": "1. Import Library",
    "section": "",
    "text": "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix"
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#a.-membangun-model-klasifikasi",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#a.-membangun-model-klasifikasi",
    "title": "1. Import Library",
    "section": "a. Membangun Model Klasifikasi",
    "text": "a. Membangun Model Klasifikasi\nSetelah memilih algoritma klasifikasi yang sesuai, langkah selanjutnya adalah melatih model menggunakan data latih.\nBerikut adalah rekomendasi tahapannya. 1. Pilih algoritma klasifikasi yang sesuai, seperti Logistic Regression, Decision Tree, Random Forest, atau K-Nearest Neighbors (KNN). 2. Latih model menggunakan data latih.\n\n# Mengimplementasikan 2 algoritma klasifikasi yang berbeda untuk membandingkan performa model.\n# Latih model\n# Model 1 - Logistic Regression\nlogreg = LogisticRegression(max_iter=500, random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Model 2 - Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\nTulis narasi atau penjelasan algoritma yang Anda gunakan.\n1. Logistic Regression Logistic Regression memberikan interpretasi yang jelas dalam memprediksi keanggotaan cluster. - Algoritma ini sederhana dan interpretatif, mudah dipahami untuk menjelaskan peluang sebuah data masuk ke klaster tertentu. - Cocok untuk kasus multi-class classification seperti prediksi klaster hasil KMeans. - Logistic Regression bekerja optimal pada data yang sudah dinormalisasi, yang sudah sesuai dengan preprocessing dataset ini. - Cocok juga untuk menguji seberapa linear hubungan antara fitur transaksi dan segmentasi cluster yang terbentuk.\n2. Random Forest Classifier Pada project ini, Random Forest dipilih karena kemampuannya dalam menangani dataset dengan kombinasi fitur numerik dan kategorikal secara efektif dengan memberi insight feature importance, serta memberi ketahanan terhadap outlier. - Dataset ini memiliki kombinasi fitur numerik dan kategorikal (contoh: TransactionAmount, CustomerAge, TransactionType, Channel). - Random Forest tahan terhadap outlier dan data yang tidak seimbang, cocok untuk dataset transaksi yang rentan mengandung anomali atau outlier akibat potensi fraud. - Selain itu, Random Forest mampu memberikan feature importance, membantu untuk memahami fitur apa saja yang paling berpengaruh dalam menentukan klaster yang berisiko (fraud) atau aman. - Secara umum, Random Forest cocok untuk data yang cukup kompleks seperti transaksi keuangan."
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#b.-evaluasi-model-klasifikasi",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#b.-evaluasi-model-klasifikasi",
    "title": "1. Import Library",
    "section": "b. Evaluasi Model Klasifikasi",
    "text": "b. Evaluasi Model Klasifikasi\nBerikut adalah rekomendasi tahapannya. 1. Lakukan prediksi menggunakan data uji. 2. Hitung metrik evaluasi seperti Accuracy dan F1-Score (Opsional: Precision dan Recall). 3. Buat confusion matrix untuk melihat detail prediksi benar dan salah.\n\n# 1. Prediksi Logistic Regression\ny_pred_logreg_train = logreg.predict(X_train)\ny_pred_logreg_test = logreg.predict(X_test)\n\n# 2. Prediksi Random Forest\ny_pred_rf_train = rf.predict(X_train)\ny_pred_rf_test = rf.predict(X_test)\n\n\n# Evaluasi Akurasi\nacc_logreg = accuracy_score(y_test, y_pred_logreg_test)\nacc_rf = accuracy_score(y_test, y_pred_rf_test)\n\nprint(f\"=== Logistic Regression Accuracy ===\\nTesting Accuracy: {acc_logreg:.4f}\")\nprint(f\"=== Random Forest Accuracy ===\\nTesting Accuracy: {acc_rf:.4f}\")\n\n# Evaluasi F1-Score\nprint(\"\\n=== Logistic Regression F1-Score ===\")\nprint(f\"F1-Score Training: {f1_score(y_train, y_pred_logreg_train, average='weighted'):.4f}\")\nprint(f\"F1-Score Testing: {f1_score(y_test, y_pred_logreg_test, average='weighted'):.4f}\")\n\nprint(\"=== Random Forest F1-Score ===\")\nprint(f\"F1-Score Training: {f1_score(y_train, y_pred_rf_train, average='weighted'):.4f}\")\nprint(f\"F1-Score Testing: {f1_score(y_test, y_pred_rf_test, average='weighted'):.4f}\")\n\n# Classification Report untuk detail\nprint(\"\\n=== Logistic Regression Classification Report ===\")\nprint(classification_report(y_test, y_pred_logreg_test))\n\nprint(\"\\n=== Random Forest Classification Report ===\")\nprint(classification_report(y_test, y_pred_rf_test))\n\n# Confusion Matrix\nprint(\"\\n=== Logistic Regression Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred_logreg_test))\n\nprint(\"\\n=== Random Forest Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred_rf_test))\n\n=== Logistic Regression Accuracy ===\nTesting Accuracy: 0.9907\n=== Random Forest Accuracy ===\nTesting Accuracy: 0.9881\n\n=== Logistic Regression F1-Score ===\nF1-Score Training: 0.9989\nF1-Score Testing: 0.9907\n=== Random Forest F1-Score ===\nF1-Score Training: 1.0000\nF1-Score Testing: 0.9881\n\n=== Logistic Regression Classification Report ===\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       569\n           1       0.98      0.98      0.98       179\n           2       1.00      1.00      1.00         6\n\n    accuracy                           0.99       754\n   macro avg       0.99      0.99      0.99       754\nweighted avg       0.99      0.99      0.99       754\n\n\n=== Random Forest Classification Report ===\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       569\n           1       0.98      0.97      0.97       179\n           2       1.00      1.00      1.00         6\n\n    accuracy                           0.99       754\n   macro avg       0.99      0.99      0.99       754\nweighted avg       0.99      0.99      0.99       754\n\n\n=== Logistic Regression Confusion Matrix ===\n[[566   3   0]\n [  4 175   0]\n [  0   0   6]]\n\n=== Random Forest Confusion Matrix ===\n[[565   4   0]\n [  5 174   0]\n [  0   0   6]]\n\n\nTulis hasil evaluasi algoritma yang digunakan, jika Anda menggunakan 2 algoritma, maka bandingkan hasilnya.\n\nEvaluasi Model Logistic Regression\n\nAkurasi Testing: 99.07%\nF1-Score Training: 99.89%\nF1-Score Testing: 99.07%\nPrecision dan Recall:\n\nKelas 0 dan kelas 2 sangat baik (hampir sempurna).\nKelas 1 sedikit lebih rendah tapi masih sangat baik (98%).\n\nConfusion Matrix:\n\nHanya 3 salah klasifikasi dari kelas 0 ke kelas 1.\n4 salah prediksi dari kelas 1 ke kelas 0.\nKelas 2 diprediksi sempurna.\n\n\n\n\nEvaluasi Model Random Forest\n\nAkurasi Testing: 98.81%\nF1-Score Training: 100%\nF1-Score Testing: 98.81%\nPrecision dan Recall:\n\nKelas 0 dan kelas 2 hampir sempurna.\nKelas 1 sedikit menurun recall-nya ke 97%.\n\nConfusion Matrix:\n\n4 salah klasifikasi dari kelas 0 ke kelas 1.\n5 salah prediksi dari kelas 1 ke kelas 0.\nKelas 2 diprediksi sempurna.\n\n\n\n\nKesimpulan\n\nLogistic Regression sedikit lebih unggul dalam akurasi dan f1-score testing dibanding Random Forest.\nRandom Forest menunjukkan overfitting karena F1-Score Training sempurna (1.0), sementara testing turun.\nLogistic Regression lebih direkomendasikan untuk dataset ini karena lebih stabil dan tidak overfitting.\nRandom Forest tetap bagus tapi perlu di-tuning lebih lanjut untuk mengurangi overfitting."
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#c.-tuning-model-klasifikasi-optional",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#c.-tuning-model-klasifikasi-optional",
    "title": "1. Import Library",
    "section": "c.¬†Tuning Model Klasifikasi (Optional)",
    "text": "c.¬†Tuning Model Klasifikasi (Optional)\nGunakan GridSearchCV, RandomizedSearchCV, atau metode lainnya untuk mencari kombinasi hyperparameter terbaik\n\nfrom sklearn.model_selection import GridSearchCV\n# Lakukan tuning model klasifikasi Random Forest, yang terdeteksi overfitting\n# Coba parameter\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7, 10],\n    'min_samples_split': [5, 10, 20],\n    'min_samples_leaf': [2, 4, 6, 8],\n    'max_features': ['sqrt', 'log2']\n}\n\n# Random Forest Classifier\nrf = RandomForestClassifier(random_state=42)\n# GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n                           cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n\n# Fit ke data training\ngrid_search.fit(X_train, y_train)\n\n# Hasil terbaik\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Accuracy Score:\", grid_search.best_score_)\n\nFitting 5 folds for each of 288 candidates, totalling 1440 fits\nBest Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\nBest Accuracy Score: 0.9863523051023051"
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#d.-evaluasi-model-klasifikasi-setelah-tuning-optional",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#d.-evaluasi-model-klasifikasi-setelah-tuning-optional",
    "title": "1. Import Library",
    "section": "d.¬†Evaluasi Model Klasifikasi setelah Tuning (Optional)",
    "text": "d.¬†Evaluasi Model Klasifikasi setelah Tuning (Optional)\nBerikut adalah rekomendasi tahapannya. 1. Gunakan model dengan hyperparameter terbaik. 2. Hitung ulang metrik evaluasi untuk melihat apakah ada peningkatan performa.\n\nbest_params = grid_search.best_params_\n# Buat ulang RF\nrf_tuned = RandomForestClassifier(**best_params, random_state=42)\nrf_tuned.fit(X_train, y_train)\n\ny_pred_rf_train = rf_tuned.predict(X_train)\ny_pred_rf_test = rf_tuned.predict(X_test)\n\n\ny_pred_rf_train = rf_tuned.predict(X_train)\ny_pred_rf_test = rf_tuned.predict(X_test)\n\nprint(\"Training Accuracy:\", accuracy_score(y_train, y_pred_rf_train))\nprint(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_rf_test))\n\nprint(\"F1-Score Training:\", f1_score(y_train, y_pred_rf_train, average='weighted'))\nprint(\"F1-Score Testing:\", f1_score(y_test, y_pred_rf_test, average='weighted'))\n\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y_test, y_pred_rf_test))\n\nprint(\"\\n=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred_rf_test))\n\nTraining Accuracy: 0.9982935153583617\nTesting Accuracy: 0.986737400530504\nF1-Score Training: 0.9982286397253537\nF1-Score Testing: 0.986737400530504\n\n=== Classification Report ===\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       569\n           1       0.97      0.97      0.97       179\n           2       1.00      1.00      1.00         6\n\n    accuracy                           0.99       754\n   macro avg       0.99      0.99      0.99       754\nweighted avg       0.99      0.99      0.99       754\n\n\n=== Confusion Matrix ===\n[[564   5   0]\n [  5 174   0]\n [  0   0   6]]"
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#e.-analisis-hasil-evaluasi-model-klasifikasi",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#e.-analisis-hasil-evaluasi-model-klasifikasi",
    "title": "1. Import Library",
    "section": "e. Analisis Hasil Evaluasi Model Klasifikasi",
    "text": "e. Analisis Hasil Evaluasi Model Klasifikasi\nBerikut adalah rekomendasi tahapannya. 1. Bandingkan hasil evaluasi sebelum dan setelah tuning (jika dilakukan). 2. Identifikasi kelemahan model, seperti: - Precision atau Recall rendah untuk kelas tertentu. - Apakah model mengalami overfitting atau underfitting? 3. Berikan rekomendasi tindakan lanjutan, seperti mengumpulkan data tambahan atau mencoba algoritma lain jika hasil belum memuaskan.\n\nHasil model Logistic Regression\nMetrik Nilai\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.99\n0.99\n0.99\n569\n\n\n1\n0.98\n0.98\n0.98\n179\n\n\n2\n1.00\n1.00\n1.00\n6\n\n\nOverall Accuracy\n\n\n0.9907\n754\n\n\n\n\nAkurasi tinggi (99.07%) baik di training maupun testing.\nGeneralisasi baik ‚Üí F1-Score train dan test seimbang, gap kecil ‚Üí tidak overfitting.\nSemua kelas diprediksi cukup baik.\n\n\n\nPerbandingan hasil evaluasi model RandomForest sebelum dan setelah tuning\n\n\n\n\n\n\n\n\nMetrik\nSebelum Tuning (Random Forest)\nSesudah Tuning (Random Forest)\n\n\n\n\nTraining Accuracy\n1.0000 (100%) ‚úÖ Indikasi Overfitting\n0.9983 (99.8%)\n\n\nTesting Accuracy\n0.9881 (98.81%)\n0.9867 (98.67%)\n\n\nF1-Score Train\n1.0000 (100%) ‚úÖ Indikasi Overfitting\n0.9982 (99.8%)\n\n\nF1-Score Test\n0.9881 (98.81%)\n0.9867 (98.67%)\n\n\nBest Params\nDefault\nmax_depth=10, max_features=‚Äòsqrt‚Äô, min_samples_leaf=2, min_samples_split=5, n_estimators=100\n\n\n\nKesimpulan perbandingan: - Setelah tuning, overfitting berkurang (train score tidak 100%). - Testing performance tetap stabil dan masih sangat baik (hampir 99%). - Model menjadi lebih generalizable, bukan sekadar menghafal data train.\n\n\n\nIdentifikasi Kelemahan Model\n\n\nLogistic Regression\n\nKelas 1 tetap punya F1-score lebih rendah dari kelas 0 dan 2 ‚Üí ini wajar karena support lebih kecil.\nKelas 2 data sangat sedikit (6 data) ‚Üí tetap rawan fluktuasi performa jika data bertambah.\n\nKelas dengan precision atau recall rendah - Secara umum, tidak ada kelemahan signifikan karena precision dan recall di atas 0.95 semua. - Namun, Kelas 1 relatif lebih rendah dibanding kelas lainnya. Ini wajar, karena biasanya kelas menengah (tidak dominan & tidak minoritas ekstrem) sering jadi yang paling susah ditebak.\n\n\nRandom Forest\nKelas dengan precision atau recall rendah Dari classification report setelah tuning: | Class | Precision | Recall | F1-Score | Support | |‚Äî‚Äî|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äì| | 0 | 0.99 | 0.99 | 0.99 | 569 | | 1 | 0.97 | 0.97 | 0.97 | 179 | | 2 | 1.00 | 1.00 | 1.00 | 6 |\nTemuan:\n- Kelas 1 (minor class) masih memiliki precision dan recall lebih rendah dibanding kelas lain (97%). - Kelas 2 jumlahnya sangat sedikit (hanya 6 data) ‚Üí ini berpotensi tidak stabil kalau data bertambah.\n\n\nOverfitting / Underfitting\n\nSebelum tuning: Model cenderung overfit ‚Üí train score sempurna (100%), test lebih rendah.\nSesudah tuning: Overfit berkurang ‚Üí gap train-test kecil, tapi train score masih tinggi.\nUnderfitting tidak terdeteksi, karena performa tinggi di kedua dataset.\n\nRekomendasi Tindakan Lanjutan\nKumpulkan Data Tambahan\n\nTambah data untuk kelas minor (kelas 2) ‚Üí hanya 6 data sangat rawan error dan bias.\nPastikan distribusi label lebih seimbang.\n\nCoba Algoritma Alternatif\n\nTes model Gradient Boosting, XGBoost, atau LightGBM yang lebih kuat di data tidak seimbang.\nAtau melakukan pengecekan SVM dengan class_weight=‚Äòbalanced‚Äô untuk melihat pengaruhnya ke kelas 1 dan 2.\n\nLakukan Cross-Validation lebih banyak\n\nSudah pakai CV=5, kalau dataset cukup besar, bisa naik ke CV=10 untuk hasil lebih stabil.\n\nFeature Engineering / Seleksi Fitur\n\nCek kembali fitur yang dipakai, mungkin masih ada fitur yang bisa meningkatkan prediksi untuk kelas minor."
  },
  {
    "objectID": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#kesimpulan-akhir",
    "href": "[Klasifikasi] Submission Akhir BMLP_Cinta Chantika Lestari.html#kesimpulan-akhir",
    "title": "1. Import Library",
    "section": "Kesimpulan Akhir",
    "text": "Kesimpulan Akhir\n\nModel hasil tuning lebih baik dan sehat, mengurangi overfitting.\n\nAkurasi dan F1-Score tetap tinggi di atas 98%.\n\nIsu utama: Kelas minor masih rawan error ‚Üí perlu perhatian lebih.\n\nNext step: Tambah data, uji algoritma lain, fokus ke penanganan imbalance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clustering and Classification: Bank Transactions Dataset",
    "section": "",
    "text": "This project explores a banking dataset to segment customers based on their transactions and then develops a predictive model for a key business outcome, such as high-value transactions or potential churn.\n\n\n\n\nDataset: https://www.kaggle.com/datasets/valakhorasani/bank-transaction-dataset-for-fraud-detection\nDescription: Contains over 2000 rows of anonymized customer transactions, including features like amount, transaction type, and time.\nPreprocessing: We created a feature engineering pipeline to convert raw transactions into behavioral features (e.g., average weekly spend, reliance on ATM).\n\n\n\n\n\n\n\nNotebook: Cluster Analysis (Unsupervised) (See the navigation bar)\nInsight: K-Means clustering identified three distinct customer segments (e.g., ‚ÄúWealthy Seniors,‚Äù ‚ÄúYoung Regulars,‚Äù ‚ÄúHigh-Risk Customers‚Äù). These segments were added to the dataset as a new feature.\n\n\n\n\n\nNotebook: Classification Model (Supervised) (See the navigation bar)\nGoal: Predict the high-value transaction flag using transaction features and the new cluster labels.\nResult: The Random Forest Classifier achieved an F1-Score of 0.88, demonstrating that the cluster feature significantly improved prediction accuracy.\n\nüëà Use the ‚ÄòAnalysis Pages‚Äô menu to dive into the full code and detailed visualizations!"
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "Clustering and Classification: Bank Transactions Dataset",
    "section": "",
    "text": "This project explores a banking dataset to segment customers based on their transactions and then develops a predictive model for a key business outcome, such as high-value transactions or potential churn."
  },
  {
    "objectID": "index.html#the-data-source",
    "href": "index.html#the-data-source",
    "title": "Clustering and Classification: Bank Transactions Dataset",
    "section": "",
    "text": "Dataset: https://www.kaggle.com/datasets/valakhorasani/bank-transaction-dataset-for-fraud-detection\nDescription: Contains over 2000 rows of anonymized customer transactions, including features like amount, transaction type, and time.\nPreprocessing: We created a feature engineering pipeline to convert raw transactions into behavioral features (e.g., average weekly spend, reliance on ATM)."
  },
  {
    "objectID": "index.html#analysis-roadmap-the-connection",
    "href": "index.html#analysis-roadmap-the-connection",
    "title": "Clustering and Classification: Bank Transactions Dataset",
    "section": "",
    "text": "Notebook: Cluster Analysis (Unsupervised) (See the navigation bar)\nInsight: K-Means clustering identified three distinct customer segments (e.g., ‚ÄúWealthy Seniors,‚Äù ‚ÄúYoung Regulars,‚Äù ‚ÄúHigh-Risk Customers‚Äù). These segments were added to the dataset as a new feature.\n\n\n\n\n\nNotebook: Classification Model (Supervised) (See the navigation bar)\nGoal: Predict the high-value transaction flag using transaction features and the new cluster labels.\nResult: The Random Forest Classifier achieved an F1-Score of 0.88, demonstrating that the cluster feature significantly improved prediction accuracy.\n\nüëà Use the ‚ÄòAnalysis Pages‚Äô menu to dive into the full code and detailed visualizations!"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html",
    "title": "1. Perkenalan Dataset",
    "section": "",
    "text": "Tahap pertama, Anda harus mencari dan menggunakan dataset tanpa label dengan ketentuan sebagai berikut:"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#a.-pembangunan-model-clustering",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#a.-pembangunan-model-clustering",
    "title": "1. Perkenalan Dataset",
    "section": "a. Pembangunan Model Clustering",
    "text": "a. Pembangunan Model Clustering\nPada tahap ini, Anda membangun model clustering dengan memilih algoritma yang sesuai untuk mengelompokkan data berdasarkan kesamaan. Berikut adalah rekomendasi tahapannya. 1. Pilih algoritma clustering yang sesuai. 2. Latih model dengan data menggunakan algoritma tersebut.\n\n# Inisialisasi dan melatih model KMeans dengan jumlah cluster = 5\nkmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\nkmeans.fit(df_cleaned)  # Latih pada kolom numerik\n\n# Mendapatkan label dan jumlah cluster\nlabels = kmeans.labels_\nk = 5\n# Menambahkan label cluster ke df_cleaned\ndf_cleaned['Cluster'] = labels\n\n# Menampilkan distribusi cluster dalam bentuk tabel\ncluster_counts = df_cleaned['Cluster'].value_counts().sort_index()\nprint(\"Distribusi Cluster:\")\nprint(cluster_counts)\n\nDistribusi Cluster:\nCluster\n0    704\n1     95\n2    362\n3    662\n4    689\nName: count, dtype: int64"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#b.-evaluasi-model-clustering",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#b.-evaluasi-model-clustering",
    "title": "1. Perkenalan Dataset",
    "section": "b. Evaluasi Model Clustering",
    "text": "b. Evaluasi Model Clustering\nUntuk menentukan jumlah cluster yang optimal dalam model clustering, Anda dapat menggunakan metode Elbow atau Silhouette Score.\nMetode ini membantu kita menemukan jumlah cluster yang memberikan pemisahan terbaik antar kelompok data, sehingga model yang dibangun dapat lebih efektif. Berikut adalah rekomendasi tahapannya. 1. Gunakan Silhouette Score dan Elbow Method untuk menentukan jumlah cluster optimal. 2. Hitung Silhouette Score sebagai ukuran kualitas cluster.\n\n# Inisialisasi supaya yg di KElbow tdk termasuk cluster\nnocluster_cols = df_cleaned.drop(columns=['Cluster']).columns\n# Menentukan jumlah cluster yang optimal dg Elbow Method\n# Inisialisasi model KMeans\nkmeans = KMeans(random_state=42, n_init=10) # Tambahkan random_state untuk reproducibility\n# Inisialisasi visualizer KElbow untuk menentukan jumlah cluster optimal\nvisualizer = KElbowVisualizer(kmeans, k=(2, 11)) # Rentang k disesuaikan (minimal 2 cluster)\n# Fit visualizer dengan data yang di-scaling\nvisualizer.fit(df_cleaned[nocluster_cols])\n\nvisualizer.show()\n\n\n\n\n\n\n\n\n\ndf_nocluster = df_cleaned.drop(columns=['Cluster'])\nsil_before = silhouette_score(df_nocluster, labels)\nprint(f\"Silhouette Score sebelum Feature Selection: {sil_before:.4f}\")\n\nSilhouette Score sebelum Feature Selection: 0.1928"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#c.-feature-selection-opsional",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#c.-feature-selection-opsional",
    "title": "1. Perkenalan Dataset",
    "section": "c.¬†Feature Selection (Opsional)",
    "text": "c.¬†Feature Selection (Opsional)\nSilakan lakukan feature selection jika Anda membutuhkan optimasi model clustering. Jika Anda menerapkan proses ini, silakan lakukan pemodelan dan evaluasi kembali menggunakan kolom-kolom hasil feature selection. Terakhir, bandingkan hasil performa model sebelum dan sesudah menerapkan feature selection.\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(threshold=0.15)\ndf_selected_variance = selector.fit_transform(df_nocluster)\n\n# Elbow method\n#sil_scores = []\n# for k in range(2, 10):\n#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n#     labels = kmeans.fit_predict(df_selected_variance)\n#     sil = silhouette_score(df_selected_variance, labels)\n#     sil_scores.append(sil)\n#     print(f\"K: {k}, Silhouette Score: {sil:.4f}\")\n\noptimal_k = 3  # Atur setelah lihat output\nkmeans_var = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nlabels_var = kmeans_var.fit_predict(df_selected_variance)\nsil_var = silhouette_score(df_selected_variance, labels_var)\nprint(f\"\\nSilhouette Score (VarianceThreshold): {sil_var:.4f}\")\n\n\nSilhouette Score (VarianceThreshold): 0.1982\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nkmeans_temp = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nlabels_temp = kmeans_temp.fit_predict(df_selected_variance)\n\nrf = RandomForestClassifier(\n    n_estimators=200,   # Tambah jumlah pohon\n    max_depth=10,       # Batasi kedalaman biar lebih general\n    min_samples_split=5,\n    random_state=42\n)\nrf.fit(df_selected_variance, labels_temp)\n\nimportances = rf.feature_importances_\nfeature_indices = np.argsort(importances)[::-1]\n\ntop_n = 4\ntop_features = feature_indices[:top_n]\ndf_selected_rf = df_selected_variance[:, top_features]\n\n# Evaluasi KMeans lagi di fitur RF\nkmeans_rf = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nlabels_rf = kmeans_rf.fit_predict(df_selected_rf)\nsil_rf = silhouette_score(df_selected_rf, labels_rf)\nprint(f\"Silhouette Score (RandomForest Feature Selection): {sil_rf:.4f}\")\n\nSilhouette Score (RandomForest Feature Selection): 0.4616\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ndf_poly = poly.fit_transform(df_selected_rf)\n\npca_poly = PCA(n_components=2)\ndf_pca_poly = pca_poly.fit_transform(df_poly)\n\nkmeans_poly = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nlabels_poly = kmeans_poly.fit_predict(df_pca_poly)\n# Evaluasi cluster final\nsil_poly = silhouette_score(df_pca_poly, labels_poly)\nprint(f\"Silhouette Score: {sil_poly:.4f}\")\n\n# Distribusi cluster\nprint(\"\\nDistribusi Cluster:\")\nprint(pd.Series(labels_poly).value_counts().sort_index())\n\nSilhouette Score: 0.7403\n\nDistribusi Cluster:\n0    1859\n1     626\n2      27\nName: count, dtype: int64"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#d.-visualisasi-hasil-clustering",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#d.-visualisasi-hasil-clustering",
    "title": "1. Perkenalan Dataset",
    "section": "d.¬†Visualisasi Hasil Clustering",
    "text": "d.¬†Visualisasi Hasil Clustering\nSetelah model clustering dilatih dan jumlah cluster optimal ditentukan, langkah selanjutnya adalah menampilkan hasil clustering melalui visualisasi.\nBerikut adalah rekomendasi tahapannya. 1. Tampilkan hasil clustering dalam bentuk visualisasi, seperti grafik scatter plot atau 2D PCA projection.\n\nprint(df.columns)\n\nIndex(['TransactionID', 'AccountID', 'TransactionAmount', 'TransactionDate',\n       'TransactionType', 'Location', 'DeviceID', 'IP Address', 'MerchantID',\n       'Channel', 'CustomerAge', 'CustomerOccupation', 'TransactionDuration',\n       'LoginAttempts', 'AccountBalance', 'PreviousTransactionDate'],\n      dtype='object')\n\n\n\n# Tambahkan cluster label hasil akhir ke dataframe asli\ndf['Cluster_Final'] = labels_poly \n\nnumerical_cols = df.select_dtypes(include=['number']).columns\nsns.pairplot(df[numerical_cols], hue='Cluster_Final', palette='viridis')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    x=df['CustomerAge'],\n    y=df['AccountBalance'],\n    hue='Cluster_Final',\n    palette='viridis',\n    data=df,  # Menggunakan df(data mentah) sebagai data\n)\nplt.title('Hasil Clustering CustomerAge vs AccountBalance')\nplt.xlabel('CustomerAge')\nplt.ylabel('AccountBalance')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    x=df['TransactionAmount'],\n    y=df['AccountBalance'],\n    hue='Cluster_Final',\n    palette='viridis',\n    data=df  # Menggunakan df (data menntah)\n)\nplt.title('Hasil Clustering TransactionAmount vs AccountBalance')\nplt.xlabel('TransactionAmount')\nplt.ylabel('AccountBalance')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Reduksi dimensi dengan PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(df[numerical_cols])\n\n# Buat DataFrame dari hasil PCA\npca_df = pd.DataFrame(data=pca_result, columns=['Principal Component 1', 'Principal Component 2'])\n\n# Tambahkan label cluster dari df_numerical ke pca_df\npca_df['Cluster_Final'] = df['Cluster_Final'].values\n\n# Buat scatter plot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    x='Principal Component 1',\n    y='Principal Component 2',\n    hue='Cluster_Final',\n    palette='viridis',\n    data=pca_df,\n)\nplt.title('Hasil Clustering K-Means (2D PCA Projection - Data Mentah)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()"
  },
  {
    "objectID": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#e.-analisis-dan-interpretasi-hasil-cluster",
    "href": "[Clustering] Submission Akhir BMLP_Cinta Chantika Lestari.html#e.-analisis-dan-interpretasi-hasil-cluster",
    "title": "1. Perkenalan Dataset",
    "section": "e. Analisis dan Interpretasi Hasil Cluster",
    "text": "e. Analisis dan Interpretasi Hasil Cluster\n\nInterpretasi Target\nTutorial: Melakukan Inverse Transform pada Data Target Setelah Clustering\nSetelah melakukan clustering dengan model KMeans, kita perlu mengembalikan data yang telah diubah (normalisasi, standarisasi, atau label encoding) ke bentuk aslinya. Berikut adalah langkah-langkahnya.\n\n1. Tambahkan Hasil Label Cluster ke DataFrame Setelah mendapatkan hasil clustering, kita tambahkan label cluster ke dalam DataFrame yang telah dinormalisasi.\ndf_normalized['Cluster'] = model_kmeans.labels_\nLakukan Inverse Transform pada feature yang sudah dilakukan Labelisasi dan Standararisasi. Berikut code untuk melakukannya: label_encoder.inverse_transform(X_Selected[[‚ÄòFitur‚Äô]])\nLalu masukkan ke dalam kolom dataset asli atau membuat dataframe baru\ndf_normalized['Fitur'] = label_encoder.inverse_transform(df_normalized[['Fitur']])\nMasukkan Data yang Sudah Di-Inverse ke dalam Dataset Asli atau Buat DataFrame Baru\ndf_original['Fitur'] = df_normalized['Fitur']\n\ndf\n\n\n\n\n\n\n\n\nTransactionID\nAccountID\nTransactionAmount\nTransactionDate\nTransactionType\nLocation\nDeviceID\nIP Address\nMerchantID\nChannel\nCustomerAge\nCustomerOccupation\nTransactionDuration\nLoginAttempts\nAccountBalance\nPreviousTransactionDate\nCluster_Final\n\n\n\n\n0\nTX000001\nAC00128\n14.09\n2023-04-11 16:29:14\nDebit\nSan Diego\nD000380\n162.198.218.92\nM015\nATM\n70\nDoctor\n81\n1\n5112.21\n2024-11-04 08:08:08\n0\n\n\n1\nTX000002\nAC00455\n376.24\n2023-06-27 16:44:19\nDebit\nHouston\nD000051\n13.149.61.4\nM052\nATM\n68\nDoctor\n141\n1\n13758.91\n2024-11-04 08:09:35\n0\n\n\n2\nTX000003\nAC00019\n126.29\n2023-07-10 18:16:08\nDebit\nMesa\nD000235\n215.97.143.157\nM009\nOnline\n19\nStudent\n56\n1\n1122.35\n2024-11-04 08:07:04\n1\n\n\n3\nTX000004\nAC00070\n184.50\n2023-05-05 16:32:11\nDebit\nRaleigh\nD000187\n200.13.225.150\nM002\nOnline\n26\nStudent\n25\n1\n8569.06\n2024-11-04 08:09:06\n0\n\n\n4\nTX000005\nAC00411\n13.45\n2023-10-16 17:51:24\nCredit\nAtlanta\nD000308\n65.164.3.100\nM091\nOnline\n26\nStudent\n198\n1\n7429.40\n2024-11-04 08:06:39\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2507\nTX002508\nAC00297\n856.21\n2023-04-26 17:09:36\nCredit\nColorado Springs\nD000625\n21.157.41.17\nM072\nBranch\n33\nDoctor\n109\n1\n12690.79\n2024-11-04 08:11:29\n0\n\n\n2508\nTX002509\nAC00322\n251.54\n2023-03-22 17:36:48\nDebit\nTucson\nD000410\n49.174.157.140\nM029\nBranch\n48\nDoctor\n177\n1\n254.75\n2024-11-04 08:11:42\n0\n\n\n2509\nTX002510\nAC00095\n28.63\n2023-08-21 17:08:50\nDebit\nSan Diego\nD000095\n58.1.27.124\nM087\nBranch\n56\nRetired\n146\n1\n3382.91\n2024-11-04 08:08:39\n0\n\n\n2510\nTX002511\nAC00118\n185.97\n2023-02-24 16:24:46\nDebit\nDenver\nD000634\n21.190.11.223\nM041\nOnline\n23\nStudent\n19\n1\n1776.91\n2024-11-04 08:12:22\n1\n\n\n2511\nTX002512\nAC00009\n243.08\n2023-02-14 16:21:23\nCredit\nJacksonville\nD000215\n59.127.135.25\nM041\nOnline\n24\nStudent\n93\n1\n131.25\n2024-11-04 08:07:49\n1\n\n\n\n\n2512 rows √ó 17 columns\n\n\n\n\ndf_cleaned\n\n\n\n\n\n\n\n\nTransactionAmount\nCustomerAge\nTransactionDuration\nLoginAttempts\nAccountBalance\nTransactionType_Credit\nTransactionType_Debit\nChannel_ATM\nChannel_Branch\nChannel_Online\nCustomerOccupation_Doctor\nCustomerOccupation_Engineer\nCustomerOccupation_Retired\nCustomerOccupation_Student\nCluster\n\n\n\n\n0\n-1.074361\n1.423718\n-0.552443\n-0.206794\n-0.000537\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n3\n\n\n1\n0.362645\n1.311287\n0.305314\n-0.206794\n2.216472\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0\n\n\n2\n-0.629153\n-1.443277\n-0.909842\n-0.206794\n-1.023534\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n4\n\n\n3\n-0.398176\n-1.049768\n-1.353017\n-0.206794\n0.885797\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0\n\n\n4\n-1.076900\n-1.049768\n1.120184\n-0.206794\n0.593589\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2507\n2.267159\n-0.656259\n-0.152156\n-0.206794\n1.942606\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n2\n\n\n2508\n-0.132163\n0.186975\n0.819969\n-0.206794\n-1.245986\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n4\n\n\n2509\n-1.016666\n0.636700\n0.376794\n-0.206794\n-0.443928\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n3\n\n\n2510\n-0.392343\n-1.218415\n-1.438793\n-0.206794\n-0.855705\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n4\n\n\n2511\n-0.165732\n-1.162199\n-0.380892\n-0.206794\n-1.277652\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n4\n\n\n\n\n2512 rows √ó 15 columns\n\n\n\n\n\nInverse Data Jika Melakukan Normalisasi/Standardisasi\nInverse Transform untuk Data yang Distandarisasi Jika data numerik telah dinormalisasi menggunakan StandardScaler atau MinMaxScaler, kita bisa mengembalikannya ke skala asli:\ndf_normalized[['Fitur_Numerik']] = scaler.inverse_transform(df_normalized[['Fitur_Numerik']])\n\ndf_cleaned['Cluster_Final'] = df['Cluster_Final'].values\ndf_cleaned.groupby('Cluster_Final').mean()\n\n\n\n\n\n\n\n\nTransactionAmount\nCustomerAge\nTransactionDuration\nLoginAttempts\nAccountBalance\nTransactionType_Credit\nTransactionType_Debit\nChannel_ATM\nChannel_Branch\nChannel_Online\nCustomerOccupation_Doctor\nCustomerOccupation_Engineer\nCustomerOccupation_Retired\nCustomerOccupation_Student\nCluster\n\n\nCluster_Final\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n-0.012949\n0.419216\n-0.004583\n-0.006816\n0.357296\n0.232921\n0.767079\n0.330285\n0.346423\n0.323292\n0.333513\n0.322754\n0.322216\n0.021517\n1.707370\n\n\n1\n0.043827\n-1.193360\n0.020537\n-0.188235\n-1.017966\n0.210863\n0.789137\n0.337061\n0.343450\n0.319489\n0.015974\n0.033546\n0.000000\n0.950479\n3.769968\n\n\n2\n-0.124597\n-1.195512\n-0.160628\n4.833578\n-0.998784\n0.111111\n0.888889\n0.296296\n0.333333\n0.370370\n0.037037\n0.148148\n0.000000\n0.814815\n1.000000\n\n\n\n\n\n\n\n\n# Inverse fitur numerik\nscaled_features = ['TransactionAmount', 'CustomerAge', 'TransactionDuration', \n                   'LoginAttempts', 'AccountBalance']\n\ndf_inverse = pd.DataFrame(scaler.inverse_transform(df_cleaned[scaled_features]), \n                          columns=scaled_features)\n\n# Inverse One-Hot Encoding\ndf_inverse['TransactionType'] = df_cleaned[['TransactionType_Credit', 'TransactionType_Debit']] \\\n                                .idxmax(axis=1).str.replace('TransactionType_', '')\ndf_inverse['Channel'] = df_cleaned[['Channel_ATM', 'Channel_Branch', 'Channel_Online']] \\\n                         .idxmax(axis=1).str.replace('Channel_', '')\ndf_inverse['CustomerOccupation'] = df_cleaned[['CustomerOccupation_Doctor', 'CustomerOccupation_Engineer',\n                                               'CustomerOccupation_Retired', 'CustomerOccupation_Student']] \\\n                                   .idxmax(axis=1).str.replace('CustomerOccupation_', '')\n\n# Re-attach cluster column\ndf_inverse['Cluster_Final'] = df_cleaned['Cluster_Final'].values\ndf_inverse.head()\n\n\n\n\n\n\n\n\nTransactionAmount\nCustomerAge\nTransactionDuration\nLoginAttempts\nAccountBalance\nTransactionType\nChannel\nCustomerOccupation\nCluster_Final\n\n\n\n\n0\n14.09\n70.0\n81.0\n1.0\n5112.21\nDebit\nATM\nDoctor\n0\n\n\n1\n376.24\n68.0\n141.0\n1.0\n13758.91\nDebit\nATM\nDoctor\n0\n\n\n2\n126.29\n19.0\n56.0\n1.0\n1122.35\nDebit\nOnline\nStudent\n1\n\n\n3\n184.50\n26.0\n25.0\n1.0\n8569.06\nDebit\nOnline\nStudent\n0\n\n\n4\n13.45\n26.0\n198.0\n1.0\n7429.40\nCredit\nOnline\nStudent\n0\n\n\n\n\n\n\n\nSetelah melakukan clustering, langkah selanjutnya adalah menganalisis karakteristik dari masing-masing cluster berdasarkan fitur yang tersedia.\nBerikut adalah rekomendasi tahapannya. 1. Analisis karakteristik tiap cluster berdasarkan fitur yang tersedia (misalnya, distribusi nilai dalam cluster). 2. Berikan interpretasi: Apakah hasil clustering sesuai dengan ekspektasi dan logika bisnis? Apakah ada pola tertentu yang bisa dimanfaatkan?\n\ndf_inverse_num = df_inverse.select_dtypes(include=['number'])\ndf_inverse_num.groupby('Cluster_Final').mean()\n\n\n\n\n\n\n\n\nTransactionAmount\nCustomerAge\nTransactionDuration\nLoginAttempts\nAccountBalance\n\n\nCluster_Final\n\n\n\n\n\n\n\n\n\n0\n281.583993\n52.131253\n119.322754\n1.120495\n6507.818392\n\n\n1\n295.892310\n23.445687\n121.079872\n1.011182\n1144.065415\n\n\n2\n253.446713\n23.407407\n108.407407\n4.037037\n1218.878148\n\n\n\n\n\n\n\n\n# Pisahkan kolom numerik dan kategorikal\nnum_cols = df_inverse.select_dtypes(include='number').columns.drop('Cluster_Final')\ncat_cols = df_inverse.select_dtypes(exclude='number').columns\n# Agregasi\nagg_num = df_inverse.groupby('Cluster_Final')[num_cols].agg(['min', 'max', 'mean'])\nagg_cat = df_inverse.groupby('Cluster_Final')[cat_cols].agg(lambda x: x.mode().iloc[0])\n# Flatten MultiIndex dari agg_num\nagg_num.columns = ['_'.join(col).strip() if col[1] else col[0] for col in agg_num.columns.values]\n\n# Gabung semua summary\ncluster_summary = pd.merge(agg_num, agg_cat, on='Cluster_Final')\nprint(cluster_summary)\n\n               TransactionAmount_min  TransactionAmount_max  \\\nCluster_Final                                                 \n0                               0.32              913.49125   \n1                               0.26              913.49125   \n2                              13.48              913.49125   \n\n               TransactionAmount_mean  CustomerAge_min  CustomerAge_max  \\\nCluster_Final                                                             \n0                          281.583993             18.0             80.0   \n1                          295.892310             18.0             36.0   \n2                          253.446713             18.0             41.0   \n\n               CustomerAge_mean  TransactionDuration_min  \\\nCluster_Final                                              \n0                     52.131253                     10.0   \n1                     23.445687                     11.0   \n2                     23.407407                     12.0   \n\n               TransactionDuration_max  TransactionDuration_mean  \\\nCluster_Final                                                      \n0                                300.0                119.322754   \n1                                299.0                121.079872   \n2                                282.0                108.407407   \n\n               LoginAttempts_min  LoginAttempts_max  LoginAttempts_mean  \\\nCluster_Final                                                             \n0                            1.0                5.0            1.120495   \n1                            1.0                2.0            1.011182   \n2                            3.0                5.0            4.037037   \n\n               AccountBalance_min  AccountBalance_max  AccountBalance_mean  \\\nCluster_Final                                                                \n0                          120.89            14977.99          6507.818392   \n1                          101.25             6568.59          1144.065415   \n2                          271.78             5335.44          1218.878148   \n\n              TransactionType Channel CustomerOccupation  \nCluster_Final                                             \n0                       Debit  Branch             Doctor  \n1                       Debit  Branch            Student  \n2                       Debit  Online            Student  \n\n\n\nsns.countplot(data=df_inverse, x='TransactionType', hue='Cluster_Final', palette= 'crest')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(data=df_inverse, x='Channel', hue='Cluster_Final', palette= 'crest')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(data=df_inverse, x='CustomerOccupation', hue='Cluster_Final', palette= 'crest')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(df_inverse, hue='Cluster_Final', palette= 'viridis')\nplt.show()\n\n\n\n\n\n\n\n\nTulis hasil interpretasinya di sini.\n\n\nüîé Analisis Karakteristik Cluster dari Model KMeans\n\nCluster 0 - ‚ÄúWealthy Seniors‚Äù / ‚ÄúHigh Net-Worth Customers‚Äù\n\nKarakteristik:\n\nUsia rata-rata tertinggi (52 tahun)\nSaldo rekening besar (6.500+)\nDidominasi profesi Doctor, Engineer, dan Retired\nLebih banyak transaksi Debit\nMemakai semua channel (ATM, Branch, Online) hampir merata\n\nInsight dan saran:\n\nSegmen pelanggan mapan, stabil secara finansial\nPotensi besar untuk cross-selling investasi, produk premium, atau asuransi pensiun\n\n\n\n\nCluster 1 - ‚ÄúYoung Regulars‚Äù / ‚ÄúActive Students‚Äù\n\nKarakteristik:\n\nUsia muda/produktif\nSaldo kecil-menengah (sekitar 1.100-an)\nMayoritas Student\nTransaksi Debit dominan\nChannel seimbang, tapi volumenya tidak setinggi Cluster 0\n\nInsight dan saran:\n\nSegmen anak muda aktif bertransaksi\nCocok untuk target produk digital banking, e-wallet, atau promo lifestyle\n\n\n\n\nCluster 2 - ‚ÄúHigh-Risk Customers‚Äù / ‚ÄúPotential Fraud Candidates‚Äù\nCluster 2 tergolong niche karena jumlah anggotanya sangat sedikit dibanding cluster lain dan memiliki karakteristik unik yang tidak umum. Meskipun usia dan saldo mirip dengan Cluster 1, perilaku mereka berbeda signifikan, terutama dari sisi login attempts yang sangat tinggi dan dominasi penggunaan channel online. Hal ini menunjukkan bahwa Cluster 2 berpotensi sebagai kelompok risky atau fraud candidates, dengan aktivitas yang tidak lazim dan lebih rawan terhadap penyalahgunaan akun. - Karakteristik: - Usia muda (23 tahun) - Login Attempts sangat tinggi (4 kali rata-rata, jauh lebih banyak dari cluster lain) - Saldo kecil-menengah - Jumlah transaksi sangat sedikit, dominasi di channel Online - Customer cenderung Student atau Engineer (jumlahnya kecil) - Risk/Fraud Insight: - Perilaku login tidak wajar (frekuensi login tinggi bisa mengindikasikan akun sering dicoba akses pihak ketiga atau user sendiri kurang paham keamanan) - Aktivitas transaksi minim; berpotensi akun nonaktif atau digunakan untuk tujuan non-transaksi biasa (seperti mule account untuk fraud) - Channel lebih dominan Online; lebih berisiko dari sisi keamanan data dan rawan cyber fraud - Rekomendasi: - Flag untuk monitoring lebih ketat - Terapkan 2FA lebih ketat - Perlu dilakukan anomaly detection di cluster ini - Campaign edukasi keamanan digital cocok untuk segmen ini"
  }
]